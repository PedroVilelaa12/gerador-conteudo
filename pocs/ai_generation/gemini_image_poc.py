#!/usr/bin/env python3
"""
POC - Gera√ß√£o de Imagens com Google Gemini/Imagen
Descri√ß√£o: POC para gerar imagens usando Google Gemini/Imagen API
Autor: Gerador de Conte√∫do
Data: 2024
"""

import os
import base64
import requests
import logging
from typing import Any, Dict, Optional
from pocs.template_poc import POCTemplate

try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

# Configurar logging
logger = logging.getLogger(__name__)


class GeminiImagePOC(POCTemplate):
    """POC para gera√ß√£o de imagens com Google Gemini/Imagen"""
    
    def __init__(self):
        """Inicializar gerador de imagens"""
        super().__init__()
        self.name = "Gemini Image Generation POC"
        self.api_key = None
        
        # Configura√ß√µes padr√£o
        self.default_size = "1024x1024"
        self.default_quality = "standard"
        self.default_style = "vivid"
        
        # Note: Google Gemini n√£o tem API de gera√ß√£o de imagens direta como DALL-E
        # Para produ√ß√£o, voc√™ precisaria usar Vertex AI Imagen ou outra solu√ß√£o
        # Este POC usa uma abordagem alternativa: Gemini para melhorar prompts
        # e ent√£o gerar imagem via outro servi√ßo ou placeholder
    
    def setup(self) -> bool:
        """Configurar conex√£o com Google Gemini API"""
        try:
            logger.info("Configurando conex√£o com Google Gemini API...")
            
            if not GEMINI_AVAILABLE:
                logger.error("Biblioteca google-generativeai n√£o instalada. Execute: poetry add google-generativeai")
                return False
            
            # Carregar API key do ambiente
            self.api_key = os.getenv('GEMINI_API_KEY')
            
            if not self.api_key:
                logger.error("GEMINI_API_KEY n√£o encontrado nas vari√°veis de ambiente")
                return False
            
            # Configurar Gemini
            genai.configure(api_key=self.api_key)
            
            logger.info("Configura√ß√£o do Gemini conclu√≠da com sucesso")
            return True
            
        except Exception as e:
            logger.error(f"Erro na configura√ß√£o do Gemini: {e}")
            return False
    
    def _get_gemini_model(self):
        """Obter modelo Gemini dispon√≠vel"""
        if not GEMINI_AVAILABLE or not self.api_key:
            return None
        
        # Lista de modelos v√°lidos (sem v1beta, usando API v1)
        # Ordem: tentar modelos mais recentes primeiro
        model_names = [
            'gemini-1.5-flash',    # Vers√£o mais r√°pida e amplamente dispon√≠vel
            'gemini-1.5-pro',      # Vers√£o mais avan√ßada
            'gemini-pro'           # Modelo padr√£o (pode estar descontinuado)
        ]
        
        for model_name in model_names:
            try:
                model = genai.GenerativeModel(model_name)
                # Retornar o modelo sem testar (o teste ser√° feito na chamada real)
                logger.debug(f"Usando modelo Gemini: '{model_name}'")
                return model
            except Exception as e:
                logger.debug(f"Modelo '{model_name}' n√£o dispon√≠vel: {e}")
                continue
        
        logger.warning("Nenhum modelo Gemini dispon√≠vel")
        return None
    
    def generate_hashtags(self, prompt: str, platform: str = "linkedin") -> str:
        """Gerar hashtags profissionais baseadas no tema usando Gemini"""
        try:
            model = self._get_gemini_model()
            
            if not model:
                # Fallback: criar hashtags b√°sicas baseadas no tema
                return self._create_fallback_hashtags(prompt)
            
            platform_hashtag_instructions = {
                "linkedin": "5-8 hashtags profissionais e relevantes, sem emojis",
                "instagram": "8-12 hashtags misturando populares e espec√≠ficas",
                "tiktok": "3-5 hashtags trending e espec√≠ficas"
            }
            
            instruction = platform_hashtag_instructions.get(platform.lower(), platform_hashtag_instructions["linkedin"])
            
            hashtag_prompt = f"""
            Gere hashtags para um post sobre: {prompt}
            
            IMPORTANTE:
            - {instruction}
            - Seja espec√≠fico e relevante ao tema
            - Use hashtags em portugu√™s quando apropriado
            - Formato: #hashtag1 #hashtag2 #hashtag3 (separadas por espa√ßo)
            - Retorne APENAS as hashtags, sem explica√ß√µes ou texto adicional
            """
            
            response = model.generate_content(hashtag_prompt)
            hashtags = response.text.strip()
            
            # Limpar e formatar
            hashtags = hashtags.strip('"').strip("'").strip()
            
            # Garantir que come√ßa com #
            lines = hashtags.split('\n')
            hashtag_list = []
            for line in lines:
                line = line.strip()
                if line.startswith('#'):
                    hashtag_list.append(line.split()[0])  # Pegar apenas a primeira palavra se tiver espa√ßo
                elif line and not line.startswith('#'):
                    # Adicionar # se n√£o tiver
                    hashtag_list.append('#' + line.split()[0])
            
            result = ' '.join(hashtag_list[:12])  # Limitar a 12 hashtags
            logger.info(f"Hashtags geradas para {platform}")
            return result
            
        except Exception as e:
            # N√£o logar erro aqui - apenas retornar fallback silenciosamente
            logger.debug(f"Erro ao gerar hashtags com Gemini (usando fallback): {e}")
            return self._create_fallback_hashtags(prompt)
    
    def _create_fallback_hashtags(self, prompt: str) -> str:
        """Criar hashtags b√°sicas quando Gemini n√£o est√° dispon√≠vel"""
        prompt_lower = prompt.lower()
        hashtags = []
        
        # Hashtags baseadas no tema
        if 'financeiro' in prompt_lower or 'mercado financeiro' in prompt_lower:
            hashtags = ['#mercadofinanceiro', '#financeiro', '#investimentos', '#economia', '#neg√≥cios', '#empreendedorismo']
        elif 'tecnologia' in prompt_lower or 'tech' in prompt_lower:
            hashtags = ['#tecnologia', '#inova√ß√£o', '#digital', '#tech', '#transforma√ß√£odigital', '#startup']
        elif 'neg√≥cios' in prompt_lower or 'business' in prompt_lower:
            hashtags = ['#neg√≥cios', '#business', '#empreendedorismo', '#sucesso', '#marketing', '#empresa']
        elif 'marketing' in prompt_lower:
            hashtags = ['#marketing', '#marketingdigital', '#publicidade', '#branding', '#comunica√ß√£o', '#m√≠dias']
        else:
            # Hashtags gen√©ricas profissionais
            hashtags = ['#profissional', '#conte√∫do', '#inova√ß√£o', '#sucesso', '#motiva√ß√£o']
        
        return ' '.join(hashtags)
    
    def generate_post_text(self, prompt: str, platform: str = "linkedin") -> str:
        """Gerar texto profissional para post em rede social usando Gemini"""
        try:
            model = self._get_gemini_model()
            
            if not model:
                # Fallback: criar texto b√°sico profissional
                return self._create_fallback_post_text(prompt)
            
            # Criar prompt espec√≠fico para cada plataforma
            platform_prompts = {
                "linkedin": f"""
                Crie um texto profissional para um post no LinkedIn sobre: {prompt}
                
                IMPORTANTE:
                - Seja profissional e engajador
                - Use linguagem adequada para LinkedIn
                - Inclua insights valiosos ou dicas pr√°ticas
                - Texto deve ter entre 100-200 palavras
                - Comece com uma frase impactante
                - Use par√°grafos curtos
                - N√£o inclua hashtags (elas ser√£o adicionadas depois)
                
                Retorne APENAS o texto do post, sem explica√ß√µes ou cita√ß√µes.
                """,
                "instagram": f"""
                Crie um texto envolvente para um post no Instagram sobre: {prompt}
                
                IMPORTANTE:
                - Seja criativo e visual
                - Use emojis de forma moderada e profissional
                - Texto deve ter entre 80-150 palavras
                - Torne o texto envolvente e interessante
                - N√£o inclua hashtags (elas ser√£o adicionadas depois)
                
                Retorne APENAS o texto do post, sem explica√ß√µes.
                """,
                "tiktok": f"""
                Crie um texto impactante e curto para um v√≠deo no TikTok sobre: {prompt}
                
                IMPORTANTE:
                - Seja direto e impactante
                - Texto deve ter entre 50-100 palavras
                - Use linguagem jovem mas profissional
                - Crie curiosidade e engajamento
                - N√£o inclua hashtags (elas ser√£o adicionadas depois)
                
                Retorne APENAS o texto, sem explica√ß√µes.
                """
            }
            
            generation_prompt = platform_prompts.get(platform.lower(), platform_prompts["linkedin"])
            
            response = model.generate_content(generation_prompt)
            post_text = response.text.strip()
            
            # Limpar o texto caso o Gemini tenha adicionado aspas ou formata√ß√£o
            post_text = post_text.strip('"').strip("'").strip()
            
            # Remover linhas vazias excessivas
            lines = [line.strip() for line in post_text.split('\n') if line.strip()]
            post_text = '\n\n'.join(lines)
            
            logger.info(f"Texto profissional gerado para {platform}")
            return post_text
            
        except Exception as e:
            # N√£o logar erro aqui - apenas retornar fallback silenciosamente
            logger.debug(f"Erro ao gerar texto profissional com Gemini (usando fallback): {e}")
            return self._create_fallback_post_text(prompt)
    
    def _create_fallback_post_text(self, prompt: str) -> str:
        """Criar texto b√°sico profissional quando Gemini n√£o est√° dispon√≠vel"""
        # Templates b√°sicos baseados no tema
        templates = {
            "mercado financeiro": "üíº O mercado financeiro est√° em constante evolu√ß√£o. √â essencial acompanhar as tend√™ncias e desenvolver uma estrat√©gia s√≥lida para navegar pelos desafios e oportunidades.\n\nSeja proativo, mantenha-se informado e construa seu conhecimento financeiro dia ap√≥s dia.",
            "financeiro": "üí∞ Entender o universo financeiro √© fundamental para tomar decis√µes inteligentes. Invista em conhecimento e esteja sempre atualizado com as melhores pr√°ticas do mercado.",
            "tecnologia": "üöÄ A tecnologia transforma nosso dia a dia e abre novas possibilidades. Estar atualizado com as inova√ß√µes tecnol√≥gicas √© essencial para prosperar no mundo moderno.",
            "neg√≥cios": "üìà Empreender requer vis√£o estrat√©gica, resili√™ncia e dedica√ß√£o. O sucesso nos neg√≥cios vem da combina√ß√£o de conhecimento, networking e execu√ß√£o consistente.",
        }
        
        prompt_lower = prompt.lower()
        for key, template in templates.items():
            if key in prompt_lower:
                return template
        
        # Template gen√©rico
        return f"üìå Explorando o tema: {prompt}\n\nConhecimento e informa√ß√£o s√£o as bases para o crescimento profissional. Mantenha-se atualizado e sempre em busca de novas oportunidades de aprendizado."
    
    def _optimize_search_query(self, prompt: str) -> str:
        """Otimizar query de busca para Pexels usando Gemini ou tradu√ß√£o manual"""
        try:
            model = self._get_gemini_model()
            optimized = None
            
            if model:
                try:
                    optimization_prompt = f"""
                    Converta o seguinte prompt em uma query de busca em ingl√™s otimizada para buscar imagens profissionais relacionadas ao tema.
                    
                    Prompt: {prompt}
                    
                    IMPORTANTE:
                    - Retorne APENAS palavras-chave relevantes em ingl√™s (2-5 palavras)
                    - Foque no tema principal
                    - Use termos comuns de busca em bancos de imagens
                    - Exemplo: "Empresa do mercado financeiro" ‚Üí "financial company business"
                    - Exemplo: "Tecnologia inovadora" ‚Üí "technology innovation"
                    
                    Retorne APENAS a query de busca, sem explica√ß√µes.
                    """
                    
                    response = model.generate_content(optimization_prompt)
                    optimized = response.text.strip()
                    
                    # Limpar a resposta caso o Gemini tenha adicionado explica√ß√µes
                    if len(optimized) > 50:
                        # Pegar apenas as primeiras palavras
                        optimized = ' '.join(optimized.split()[:5])
                    
                    # Validar se a resposta parece v√°lida (cont√©m palavras em ingl√™s, n√£o erro)
                    if optimized and len(optimized.split()) >= 2:
                        logger.info(f"Query otimizada pelo Gemini: {optimized}")
                        return optimized
                    else:
                        logger.warning("Resposta do Gemini inv√°lida, usando tradu√ß√£o manual")
                except Exception as e:
                    logger.warning(f"Erro ao usar Gemini para otimizar query: {e}")
            
            # Fallback: tradu√ß√£o manual - SEMPRE executar se Gemini falhar
            logger.info("Usando tradu√ß√£o manual para otimizar query...")
            translations = {
                'mercado financeiro': ['financial', 'market', 'business'],
                'empresa do mercado financeiro': ['financial', 'company', 'business', 'corporate'],
                'empresa financeira': ['financial', 'company', 'business'],
                'mercado': ['financial', 'market'],
                'financeiro': ['financial', 'finance'],
                'empresa': ['company', 'business'],
                'empresa do': ['company', 'business'],
                'neg√≥cios': ['business', 'corporate'],
                'tecnologia': ['technology', 'innovation'],
                'tech': ['technology'],
                'marketing': ['marketing', 'advertising'],
                'design': ['design', 'creative'],
                'sa√∫de': ['health', 'medical'],
                'educa√ß√£o': ['education', 'learning'],
                'inova√ß√£o': ['innovation', 'technology'],
                'startup': ['startup', 'business'],
                'corporativo': ['corporate', 'business'],
                'profissional': ['professional', 'business']
            }
            
            prompt_lower = prompt.lower().strip()
            keywords = []
            
            # Buscar termos mais longos primeiro (para pegar "empresa do mercado financeiro" antes de termos menores)
            sorted_translations = sorted(translations.items(), key=lambda x: len(x[0]), reverse=True)
            
            remaining_prompt = prompt_lower
            for pt_term, en_words in sorted_translations:
                if pt_term in remaining_prompt:
                    keywords.extend(en_words)
                    # Remover o termo encontrado para evitar duplicatas
                    remaining_prompt = remaining_prompt.replace(pt_term, '', 1)
                    logger.debug(f"Termo encontrado: '{pt_term}' ‚Üí {en_words}")
            
            if keywords:
                # Remover duplicatas mantendo ordem
                unique_keywords = []
                seen = set()
                for kw in keywords:
                    if kw.lower() not in seen:
                        unique_keywords.append(kw.lower())
                        seen.add(kw.lower())
                
                # Limitar a 5 palavras-chave mais relevantes
                result = ' '.join(unique_keywords[:5])
                logger.info(f"‚úÖ Query traduzida manualmente: '{result}'")
                return result
            
            # Se n√£o encontrou tradu√ß√£o, tentar tradu√ß√£o simples palavra por palavra
            word_map = {
                'do': None, 'da': None, 'de': None, 'e': None, 'o': None, 'a': None, 'os': None, 'as': None,
                'empresa': 'company',
                'mercado': 'market',
                'financeiro': 'financial',
                'neg√≥cios': 'business',
                'tecnologia': 'technology',
                'inova√ß√£o': 'innovation',
                'marketing': 'marketing'
            }
            
            words = prompt_lower.split()
            translated_words = []
            for w in words:
                translated = word_map.get(w)
                if translated:
                    translated_words.append(translated)
            
            if translated_words:
                result = ' '.join(translated_words[:5])
                logger.info(f"‚úÖ Query traduzida palavra por palavra: '{result}'")
                return result
            
            # Se tudo falhar, retornar uma busca gen√©rica baseada no contexto
            logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel traduzir '{prompt}'. Usando busca gen√©rica.")
            # Nunca usar o prompt em portugu√™s no Pexels
            return "business professional"
                
        except Exception as e:
            logger.warning(f"Erro ao otimizar query: {e}. Usando busca gen√©rica.")
            # NUNCA retornar prompt em portugu√™s - sempre usar fallback em ingl√™s
            return "business professional"
    
    def generate_image(self, prompt: str, size: str = None, quality: str = None, style: str = None) -> Dict[str, Any]:
        """
        Gerar imagem usando Google Gemini/Imagen
        
        NOTA: O Google Gemini n√£o possui uma API p√∫blica direta de gera√ß√£o de imagens como o DALL-E.
        Este m√©todo usa uma abordagem alternativa:
        1. Usa Gemini para melhorar/enriquecer o prompt
        2. Para produ√ß√£o, voc√™ precisaria integrar com Vertex AI Imagen ou outro servi√ßo
        
        Para desenvolvimento/teste, retornamos um placeholder ou imagem gerada via outro m√©todo.
        """
        try:
            logger.info(f"Gerando imagem com prompt: {prompt}")
            
            # Usar configura√ß√µes padr√£o se n√£o especificadas
            size = size or self.default_size
            quality = quality or self.default_quality
            style = style or self.default_style
            
            # Passo 1: Usar Gemini para melhorar o prompt (opcional)
            # Se falhar, o sistema continua com o prompt original
            improved_prompt = prompt
            model = self._get_gemini_model()
            
            if model:
                try:
                    improvement_prompt = f"""
                    Melhore o seguinte prompt para gera√ß√£o de imagem, tornando-o mais detalhado e visual:
                    
                    Prompt original: {prompt}
                    
                    Retorne APENAS o prompt melhorado, sem explica√ß√µes adicionais.
                    """
                    
                    response = model.generate_content(improvement_prompt)
                    improved_prompt = response.text.strip()
                    
                    logger.info(f"Prompt melhorado pelo Gemini: {improved_prompt}")
                except Exception as e:
                    logger.warning(f"Erro ao melhorar prompt com Gemini: {e}. Usando prompt original.")
                    improved_prompt = prompt
            
            # Passo 2: Gerar ou buscar imagem profissional relacionada ao tema
            image_bytes = None
            
            # Op√ß√£o 1: Tentar buscar imagem do Pexels (gratuito, profissional)
            pexels_api_key = os.getenv('PEXELS_API_KEY')
            if pexels_api_key:
                try:
                    logger.info("Tentando buscar imagem relacionada ao tema no Pexels...")
                    
                    # Calcular varia√ß√£o baseada no timestamp para variar imagens
                    # Usa timestamp + hash do prompt para garantir variedade mesmo com mesmo tema
                    import time
                    import hashlib
                    
                    # Criar uma "assinatura" √∫nica baseada no prompt e timestamp
                    prompt_hash = int(hashlib.md5(prompt.lower().encode()).hexdigest()[:8], 16)
                    timestamp_seconds = int(time.time())
                    
                    # Combinar ambos para criar varia√ß√£o, mas que mude ao longo do tempo
                    variation_seed = (prompt_hash + timestamp_seconds) % 10  # Varia entre 0-9
                    
                    logger.info(f"Usando varia√ß√£o {variation_seed} para tema '{prompt}'")
                    
                    # Usar prompt original para busca (melhor para tradu√ß√£o)
                    image_bytes = self._search_image_from_pexels(prompt, size, variation_seed)
                    if image_bytes:
                        logger.info("‚úÖ Imagem encontrada no Pexels")
                    else:
                        logger.warning("Nenhuma imagem encontrada no Pexels para este tema")
                except Exception as e:
                    logger.warning(f"Erro ao buscar imagem no Pexels: {e}")
            
            # Op√ß√£o 2: Criar placeholder profissional relacionado ao tema
            if not image_bytes:
                logger.info("Gerando imagem placeholder profissional relacionada ao tema...")
                image_bytes = self._generate_placeholder_image(improved_prompt, size)
            
            # Passo 3: Gerar texto profissional para post
            post_text = None
            hashtags = None
            try:
                logger.info("Gerando texto profissional para post...")
                post_text = self.generate_post_text(prompt, "linkedin")
                
                logger.info("Gerando hashtags profissionais...")
                hashtags = self.generate_hashtags(prompt, "linkedin")
            except Exception as e:
                logger.warning(f"Erro ao gerar conte√∫do de texto: {e}")
            
            logger.info("Imagem gerada com sucesso")
            return {
                "status": "success",
                "message": "Imagem gerada com sucesso",
                "data": {
                    "image_bytes": image_bytes,
                    "prompt": prompt,
                    "improved_prompt": improved_prompt,
                    "size": size,
                    "quality": quality,
                    "style": style,
                    "revised_prompt": improved_prompt,  # Compatibilidade com c√≥digo existente
                    "post_text": post_text,  # Texto profissional gerado
                    "hashtags": hashtags  # Hashtags profissionais geradas
                }
            }
                
        except Exception as e:
            logger.error(f"Erro na gera√ß√£o de imagem: {e}")
            return {
                "status": "error",
                "message": f"Erro ao gerar imagem: {str(e)}",
                "data": {}
            }
    
    def _search_image_from_pexels(self, prompt: str, size: str, image_variation: int = 0) -> Optional[bytes]:
        """
        Buscar imagem profissional relacionada ao tema no Pexels
        
        Args:
            prompt: Tema da busca
            size: Tamanho da imagem
            image_variation: √çndice para variar a imagem (0 = primeira, 1 = segunda, etc)
        """
        try:
            pexels_api_key = os.getenv('PEXELS_API_KEY')
            if not pexels_api_key:
                return None
            
            # Usar Gemini para criar uma query de busca otimizada em ingl√™s
            search_query = self._optimize_search_query(prompt)
            
            # Limitar tamanho da query para busca
            if len(search_query) > 100:
                search_query = search_query[:100]
            
            # Buscar imagem no Pexels
            url = "https://api.pexels.com/v1/search"
            headers = {
                "Authorization": pexels_api_key
            }
            # Determinar orienta√ß√£o baseado no tamanho
            orientation = "square"
            if "x" in size:
                try:
                    width, height = map(int, size.split('x'))
                    if width > height:
                        orientation = "landscape"
                    elif height > width:
                        orientation = "portrait"
                except:
                    orientation = "square"
            
            # Buscar m√∫ltiplas imagens para permitir varia√ß√£o
            per_page = max(5, image_variation + 1)  # Buscar pelo menos o suficiente para a varia√ß√£o desejada
            
            params = {
                "query": search_query,
                "per_page": per_page,
                "orientation": orientation,
                "page": 1  # Sempre buscar da primeira p√°gina, mas variar qual imagem pegar
            }
            
            response = requests.get(url, headers=headers, params=params, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                photos = data.get("photos", [])
                
                logger.info(f"Pexels retornou {len(photos)} imagens para a query: '{search_query}'")
                
                if photos:
                    # Variar qual imagem usar baseado em image_variation
                    # Usar m√≥dulo para garantir que n√£o ultrapasse o array
                    photo_index = image_variation % len(photos)
                    photo = photos[photo_index]
                    
                    if image_variation > 0:
                        logger.info(f"Usando varia√ß√£o {photo_index + 1} de {len(photos)} imagens dispon√≠veis")
                    photo_url = photo.get("src", {}).get("large2x") or \
                               photo.get("src", {}).get("large") or \
                               photo.get("src", {}).get("medium") or \
                               photo.get("src", {}).get("original")
                    
                    if photo_url:
                        # Baixar a imagem
                        img_response = requests.get(photo_url, timeout=15)
                        if img_response.status_code == 200:
                            # Redimensionar se necess√°rio usando Pillow
                            try:
                                from PIL import Image
                                import io
                                
                                img = Image.open(io.BytesIO(img_response.content))
                                
                                # Redimensionar para o tamanho solicitado
                                target_width, target_height = map(int, size.split('x'))
                                img_resized = img.resize((target_width, target_height), Image.Resampling.LANCZOS)
                                
                                # Converter para bytes
                                img_byte_arr = io.BytesIO()
                                img_resized.save(img_byte_arr, format='PNG')
                                return img_byte_arr.getvalue()
                            except ImportError:
                                # Se Pillow n√£o estiver dispon√≠vel, retornar imagem original
                                return img_response.content
                            
            logger.warning("Nenhuma imagem encontrada no Pexels para o tema")
            return None
            
        except Exception as e:
            logger.warning(f"Erro ao buscar imagem no Pexels: {e}")
            return None
    
    def _generate_placeholder_image(self, prompt: str, size: str) -> bytes:
        """
        Gerar imagem placeholder profissional relacionada ao tema
        Cria uma imagem visualmente atraente baseada nas palavras-chave do prompt
        """
        try:
            from PIL import Image, ImageDraw, ImageFont, ImageFilter
            import io
            
            width, height = map(int, size.split('x'))
            
            # Extrair palavras-chave principais do prompt
            keywords = prompt.lower().split()[:5]  # Pegar at√© 5 palavras principais
            main_keyword = keywords[0] if keywords else "conte√∫do"
            
            # Criar gradiente de cores baseado no tema
            # Cores profissionais para diferentes temas
            color_schemes = {
                'financeiro': ['#1e3a5f', '#2d5aa0', '#4a90e2', '#87ceeb'],
                'mercado': ['#1e3a5f', '#2d5aa0', '#4a90e2', '#87ceeb'],  # Alias para financeiro
                'financeira': ['#1e3a5f', '#2d5aa0', '#4a90e2', '#87ceeb'],
                'tecnologia': ['#0f3460', '#16213e', '#533483', '#e94560'],
                'tech': ['#0f3460', '#16213e', '#533483', '#e94560'],
                'neg√≥cios': ['#2c3e50', '#34495e', '#3498db', '#2980b9'],
                'business': ['#2c3e50', '#34495e', '#3498db', '#2980b9'],
                'marketing': ['#c0392b', '#e74c3c', '#ec7063', '#f1948a'],
                'design': ['#8e44ad', '#9b59b6', '#bb8fce', '#d7bde2'],
                'sa√∫de': ['#27ae60', '#2ecc71', '#58d68d', '#82e0aa'],
                'health': ['#27ae60', '#2ecc71', '#58d68d', '#82e0aa'],
                'educa√ß√£o': ['#f39c12', '#f4d03f', '#f7dc6f', '#f9e79f'],
                'education': ['#f39c12', '#f4d03f', '#f7dc6f', '#f9e79f'],
            }
            
            # Escolher esquema de cores baseado no prompt
            prompt_lower = prompt.lower()
            colors = ['#3498db', '#2ecc71', '#9b59b6', '#e74c3c']  # Default
            for key, scheme in color_schemes.items():
                if key in prompt_lower:
                    colors = scheme
                    break
            
            # Criar imagem com gradiente
            img = Image.new('RGB', (width, height), color=colors[0])
            draw = ImageDraw.Draw(img)
            
            # Desenhar gradiente circular ou retangular
            num_steps = 50
            center_x, center_y = width // 2, height // 2
            max_radius = int((width ** 2 + height ** 2) ** 0.5)
            
            for i in range(num_steps):
                alpha = i / num_steps
                # Interpolar entre cores
                color_idx = int(alpha * (len(colors) - 1))
                next_idx = min(color_idx + 1, len(colors) - 1)
                local_alpha = (alpha * (len(colors) - 1)) % 1
                
                # Converter hex para RGB
                def hex_to_rgb(h):
                    return tuple(int(h[j:j+2], 16) for j in (1, 3, 5))
                
                rgb1 = hex_to_rgb(colors[color_idx])
                rgb2 = hex_to_rgb(colors[next_idx])
                rgb = tuple(int(rgb1[k] * (1 - local_alpha) + rgb2[k] * local_alpha) for k in range(3))
                
                radius = int(max_radius * alpha)
                draw.ellipse(
                    [center_x - radius, center_y - radius, 
                     center_x + radius, center_y + radius],
                    fill=rgb, outline=None
                )
            
            # Adicionar texto estilizado
            try:
                # Tentar usar fonte maior se dispon√≠vel
                font_size = min(width // 15, height // 15, 48)
                try:
                    font = ImageFont.truetype("arial.ttf", font_size)
                except:
                    try:
                        font = ImageFont.truetype("/System/Library/Fonts/Helvetica.ttc", font_size)
                    except:
                        font = ImageFont.load_default()
            except:
                font = ImageFont.load_default()
            
            # Texto principal (tema)
            main_text = main_keyword.upper()
            bbox = draw.textbbox((0, 0), main_text, font=font)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]
            
            # Adicionar sombra ao texto para melhor legibilidade
            shadow_offset = 2
            draw.text(
                ((width - text_width) // 2 + shadow_offset, (height - text_height) // 2 + shadow_offset),
                main_text, fill='#000000', font=font, align='center'
            )
            draw.text(
                ((width - text_width) // 2, (height - text_height) // 2),
                main_text, fill='#ffffff', font=font, align='center'
            )
            
            # Aplicar filtro de blur sutil para suavizar
            try:
                img = img.filter(ImageFilter.GaussianBlur(radius=1))
            except:
                pass
            
            # Converter para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG', quality=95)
            img_byte_arr = img_byte_arr.getvalue()
            
            logger.info(f"Imagem placeholder profissional criada (tamanho: {width}x{height}, tema: {main_keyword})")
            return img_byte_arr
            
        except ImportError:
            # Se Pillow n√£o estiver dispon√≠vel, criar uma imagem m√≠nima em bytes
            logger.warning("Pillow n√£o dispon√≠vel. Criando placeholder m√≠nimo.")
            # Retornar uma imagem PNG m√≠nima v√°lida (1x1 pixel transparente)
            minimal_png = base64.b64decode(
                'iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNk+M9QDwADhgGAWjR9awAAAABJRU5ErkJggg=='
            )
            return minimal_png
        except Exception as e:
            logger.error(f"Erro ao criar placeholder: {e}")
            raise
    
    def save_image(self, image_bytes: bytes, filename: str, output_dir: str = "generated_images") -> str:
        """Salvar imagem em arquivo"""
        try:
            # Criar diret√≥rio se n√£o existir
            os.makedirs(output_dir, exist_ok=True)
            
            # Caminho completo do arquivo
            filepath = os.path.join(output_dir, filename)
            
            # Salvar imagem
            with open(filepath, 'wb') as f:
                f.write(image_bytes)
            
            logger.info(f"Imagem salva em: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Erro ao salvar imagem: {e}")
            return ""
    
    def run(self) -> Dict[str, Any]:
        """Executar gera√ß√£o de imagem de exemplo"""
        try:
            logger.info("Executando gera√ß√£o de imagem de exemplo...")
            
            # Prompt de exemplo
            test_prompt = "A futuristic robot creating digital art in a modern studio, high quality, detailed"
            
            # Gerar imagem
            result = self.generate_image(test_prompt)
            
            if result["status"] == "success":
                # Salvar imagem
                filename = f"generated_image_{int(os.urandom(4).hex(), 16)}.png"
                filepath = self.save_image(
                    result["data"]["image_bytes"], 
                    filename
                )
                
                if filepath:
                    result["data"]["filepath"] = filepath
                    result["data"]["filename"] = filename
                
                logger.info("Gera√ß√£o de imagem conclu√≠da com sucesso")
                return result
            else:
                logger.error(f"Falha na gera√ß√£o: {result['message']}")
                return result
            
        except Exception as e:
            logger.error(f"Erro na execu√ß√£o: {e}")
            return {
                "status": "error",
                "message": str(e),
                "data": {}
            }
    
    def cleanup(self):
        """Limpar recursos"""
        try:
            logger.info("Limpando recursos do Gemini...")
            # Aqui voc√™ poderia limpar arquivos tempor√°rios, etc.
            logger.info("Limpeza do Gemini conclu√≠da")
        except Exception as e:
            logger.error(f"Erro na limpeza: {e}")


def main():
    """Fun√ß√£o principal"""
    poc = GeminiImagePOC()
    
    try:
        if not poc.setup():
            logger.error("Falha na configura√ß√£o do Gemini")
            return
        
        result = poc.run()
        
        print(f"\nResultado da POC - Gera√ß√£o de Imagem Gemini:")
        print(f"Status: {result['status']}")
        print(f"Mensagem: {result['message']}")
        
        if result['status'] == 'success' and 'data' in result:
            print(f"\nDetalhes da gera√ß√£o:")
            print(f"  Prompt: {result['data'].get('prompt', 'N/A')}")
            print(f"  Prompt melhorado: {result['data'].get('improved_prompt', 'N/A')}")
            print(f"  Tamanho: {result['data'].get('size', 'N/A')}")
            print(f"  Qualidade: {result['data'].get('quality', 'N/A')}")
            print(f"  Estilo: {result['data'].get('style', 'N/A')}")
            print(f"  Arquivo salvo: {result['data'].get('filepath', 'N/A')}")
        
        print("\n‚ö†Ô∏è  NOTA IMPORTANTE:")
        print("O Google Gemini n√£o possui uma API p√∫blica de gera√ß√£o de imagens como o DALL-E.")
        print("Este POC usa um placeholder. Para produ√ß√£o, considere:")
        print("  1. Usar Vertex AI Imagen (requer Google Cloud)")
        print("  2. Integrar com outro servi√ßo de gera√ß√£o de imagens")
        print("  3. Usar Gemini apenas para melhorar prompts")
        
    except Exception as e:
        logger.error(f"Erro inesperado: {e}")
    
    finally:
        poc.cleanup()


if __name__ == "__main__":
    main()

